{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekwEPS9C9ngs"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('MyDrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp-Z_fFCDrWJ"
      },
      "outputs": [],
      "source": [
        "!pip install 'keras<3.0.0' mediapipe-model-maker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K57gfRSqHROk"
      },
      "outputs": [],
      "source": [
        "!pip install typeguard>=4.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZw57VAC9z49"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle/\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d vishnukv64/cvcclinicdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnu3PCAj91qp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c6fd96-65ba-46f2-9888-173f544d6849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace CVC-ClinicDB/Ground_Truth/1.tif? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ],
      "source": [
        "!unzip -q /content/cvcclinicdb.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrMnNHGB95oC",
        "outputId": "edf9e159-f02b-439a-c88f-e02beb30b7a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (8.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install natsort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80DVMoMG98xY",
        "outputId": "fdc60bfc-da75-42f2-ff81-de0ed38ef6b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.17 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import natsort\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import glob\n",
        "from PIL import Image\n",
        "import albumentations\n",
        "from sklearn import model_selection\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import cv2\n",
        "import helper\n",
        "from PIL import ImageFile\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbZOcJmY-B73",
        "outputId": "e4fcced0-ebcd-4b84-db4d-8e8a095050d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "612\n",
            "612\n"
          ]
        }
      ],
      "source": [
        "Data_dir_images = \"CVC-ClinicDB/Original\"\n",
        "Data_dir_masks = \"CVC-ClinicDB/Ground_Truth\"\n",
        "\n",
        "\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "batch_size = 2\n",
        "\n",
        "# class_names = [\"polyp\"]\n",
        "\n",
        "images_list = glob.glob(f\"{Data_dir_images}/*\")\n",
        "images_list = natsort.natsorted(images_list, reverse = False)\n",
        "print(len(images_list))\n",
        "masks_list = glob.glob(f\"{Data_dir_masks}/*\")\n",
        "masks_list = natsort.natsorted(masks_list, reverse = False)\n",
        "print(len(masks_list))\n",
        "\n",
        "split=0.1\n",
        "total_size = len(images_list)\n",
        "valid_size = int(split * total_size)\n",
        "test_size = int(split * total_size)\n",
        "\n",
        "train_img, valid_img , train_masks, valid_masks = model_selection.train_test_split(images_list, masks_list, test_size=valid_size, random_state = 42)\n",
        "\n",
        "train_img, test_img , train_masks, test_masks = model_selection.train_test_split(train_img, train_masks, test_size=test_size, random_state = 42)\n",
        "\n",
        "train_img_tunner, valid_img_tunner , train_masks_tunner, valid_masks_tunner = model_selection.train_test_split(images_list, masks_list, test_size=valid_size, random_state = 42)\n",
        "\n",
        "train_img_tunner, test_img_tunner , train_mask_tunners, test_masks_tunner = model_selection.train_test_split(train_img_tunner, train_masks_tunner, test_size=test_size, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data_tunner(images, masks):\n",
        "    final_images, final_masks = list(), list()\n",
        "    for e, item in enumerate(images):\n",
        "        image = cv2.imread(images[e], cv2.IMREAD_GRAYSCALE) # , cv2.IMREAD_GRAYSCALE#we use cv2 because PIL didnt load the image and we convert later to PIL image\n",
        "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        final_images.append(image)\n",
        "\n",
        "        mask = cv2.imread(masks[e], cv2.IMREAD_GRAYSCALE)#we use cv2 because PIL didnt load the image and we convert later to PIL image\n",
        "        # mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
        "        final_masks.append(mask)\n",
        "\n",
        "    return np.array(final_images), np.array(final_masks)"
      ],
      "metadata": {
        "id": "p3B0TVwEQISj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j31klj9p-PJG"
      },
      "outputs": [],
      "source": [
        "train_images_tunner, train_masks_tunner = read_data_tunner(train_img_tunner, train_masks_tunner)\n",
        "valid_images_tunner, valid_masks_tunner = read_data_tunner(valid_img_tunner, valid_masks_tunner)\n",
        "test_images_tunner, test_masks_tunner = read_data_tunner(test_img_tunner, test_masks_tunner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svmeECxF-QHq",
        "outputId": "1224c0d8-64ad-471f-e190-5adddd9c5931"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(490, 288, 384)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train_images_tunner.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNvA1HBx-Rle"
      },
      "outputs": [],
      "source": [
        "# converting all mask pixels other than 0 to 1\n",
        "for e,mask in enumerate(train_masks_tunner):\n",
        "    train_masks_tunner[e][train_masks_tunner[e]>0] = 1\n",
        "\n",
        "for e,mask in enumerate(valid_masks_tunner):\n",
        "    valid_masks_tunner[e][valid_masks_tunner[e]>0] = 1\n",
        "\n",
        "for e,mask in enumerate(test_masks_tunner):\n",
        "    test_masks_tunner[e][test_masks_tunner[e]>0] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJ2J6SJL-WJ3"
      },
      "outputs": [],
      "source": [
        "# image size\n",
        "SIZE_X = 384\n",
        "SIZE_Y = 288\n",
        "n_classes = 2  # class 1 is background, class 2 is of polyp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTfWs8x_-YER"
      },
      "outputs": [],
      "source": [
        "# adding 3rd dimension for unet model\n",
        "# normalizing image with keras normalizer\n",
        "# it divides each pixel with 255 to change pixel between 0-1\n",
        "#\n",
        "train_images_tunner = np.expand_dims(train_images_tunner, axis=3)\n",
        "# train_images = normalize(train_images, axis=1)\n",
        "\n",
        "valid_images_tunner = np.expand_dims(valid_images_tunner, axis=3)\n",
        "# valid_images = normalize(valid_images, axis=1)\n",
        "\n",
        "test_images_tunner = np.expand_dims(test_images_tunner, axis=3)\n",
        "# test_images = normalize(test_images, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TmhZyRG-ZH4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBjdJx3i-bT5"
      },
      "outputs": [],
      "source": [
        "train_masks_cat_tunner = to_categorical(train_masks_tunner, num_classes=n_classes)\n",
        "test_masks_cat_tunner = to_categorical(test_masks_tunner, num_classes=n_classes)\n",
        "valid_masks_cat_tunner = to_categorical(valid_masks_tunner, num_classes=n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVeZkgop-eMP"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.backend import cast\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pe0bfxJQ-f-J"
      },
      "outputs": [],
      "source": [
        "def focalLoss(alpha=0.2, gamma=3, rate=0.1, weights=0, bias=0):\n",
        "    def customloss(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "  #     y_pred = tf.cast(y_pred, tf.float64)\n",
        "\n",
        "        # bias = 0\n",
        "        weighted_sum = tf.math.abs(tf.cast(weights*gamma + bias, tf.float32))\n",
        "        # weighted_sum = tf.math.abs(tf.cast(weights*gamma + bias + gamma, tf.float32))\n",
        "        #weighted_sum = tf.math.abs(tf.cast(weights + bias + gamma, tf.float32)) # algo 1\n",
        "        y_predicted = tf.math.abs(tf.nn.sigmoid(weighted_sum))\n",
        "\n",
        "        gamma_d = tf.math.reduce_mean(y_predicted-y_true)\n",
        "        bias_d = tf.math.reduce_mean(y_predicted-y_true)\n",
        "\n",
        "        gamma_ = tf.math.round(gamma - rate * gamma_d)\n",
        "\n",
        "        ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "                labels=y_true, logits=y_pred)\n",
        "        alpha1 = y_true * alpha + (1. - y_true) * (1. - alpha)\n",
        "\n",
        "        pt = tf.where(y_true==1, x=y_pred, y=1-y_pred)\n",
        "        F_loss =  alpha1 * (1. - pt) ** gamma_ * ce\n",
        "        return tf.reduce_mean(F_loss)\n",
        "    return customloss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJMuEyPu-pnX"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def get_iou_vector(A, B):\n",
        "    t = A>0\n",
        "    p = B>0\n",
        "    intersection = np.logical_and(t,p)\n",
        "    union = np.logical_or(t,p)\n",
        "    iou = (np.sum(intersection) + 1e-10 )/ (np.sum(union) + 1e-10)\n",
        "    return iou\n",
        "\n",
        "\n",
        "def check_units(y_true, y_pred):\n",
        "    if y_pred.shape[1] != 1:\n",
        "      y_pred = y_pred[:,1:2]\n",
        "      y_true = y_true[:,1:2]\n",
        "    return y_true, y_pred\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    y_true, y_pred = check_units(y_true, y_pred)\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    y_true, y_pred = check_units(y_true, y_pred)\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def iou_metric(label, pred):\n",
        "    return tf.numpy_function(get_iou_vector, [label, pred>0.5], tf.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6eDl4Pc-qnE"
      },
      "outputs": [],
      "source": [
        "!pip install keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rllCOVTA-sl4"
      },
      "outputs": [],
      "source": [
        "import keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_sLSHb5D2Is"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, BatchNormalization, MaxPooling2D, Conv2DTranspose, concatenate\n",
        "from keras.layers import Conv2D\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0R_bDGGr-v9h"
      },
      "outputs": [],
      "source": [
        "# from keras.layers import Input, BatchNormalization, MaxPooling2D, Conv2DTranspose, concatenate\n",
        "# from keras.layers.convolutional import Conv2D\n",
        "# from keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gsUdjC4-1ne"
      },
      "outputs": [],
      "source": [
        "def base_model(hp):\n",
        "    in_layer = Input(shape=(None, None, 1), dtype='float32', name='input_1')\n",
        "\n",
        "    c1 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(in_layer)\n",
        "    c1 = BatchNormalization()(c1)\n",
        "    c1 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(c1)\n",
        "    c1 = BatchNormalization()(c1)\n",
        "    p1 = MaxPooling2D(pool_size=(2, 2),padding='valid',strides=(2, 2))(c1)\n",
        "\n",
        "    c2 = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(p1)\n",
        "    c2 = BatchNormalization()(c2)\n",
        "    c2 = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(c2)\n",
        "    c2 = BatchNormalization()(c2)\n",
        "    p2 = MaxPooling2D(pool_size=(2, 2),padding='valid',strides=(2, 2))(c2)\n",
        "\n",
        "    c3 = Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(p2)\n",
        "    c3 = BatchNormalization()(c3)\n",
        "    c3 = Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(c3)\n",
        "    c3 = BatchNormalization()(c3)\n",
        "    p3 = MaxPooling2D(pool_size=(2, 2),padding='valid',strides=(2, 2))(c3)\n",
        "\n",
        "    c4 = Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(p3)\n",
        "    c4 = BatchNormalization()(c4)\n",
        "    c4 = Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(c4)\n",
        "    c4 = BatchNormalization()(c4)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2),padding='valid',strides=(2, 2))(c4)\n",
        "\n",
        "    c5 = Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(p4)\n",
        "    c5 = BatchNormalization()(c5)\n",
        "    c5 = Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(c5)\n",
        "    c5 = BatchNormalization()(c5)\n",
        "    x5 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    x5 = concatenate([x5,c4], axis=-1)\n",
        "\n",
        "    c6 = Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(x5)\n",
        "    c6 = BatchNormalization()(c6)\n",
        "    c6 = Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(c6)\n",
        "    c6 = BatchNormalization()(c6)\n",
        "    x6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    x6 = concatenate([x6,c3], axis=-1)\n",
        "\n",
        "    c7 = Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(x6)\n",
        "    c7 = BatchNormalization()(c7)\n",
        "    c7 = Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(c7)\n",
        "    c7 = BatchNormalization()(c7)\n",
        "    x7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    x7 = concatenate([x7,c2], axis=-1)\n",
        "\n",
        "    c8 = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(x7)\n",
        "    c8 = BatchNormalization()(c8)\n",
        "    c8 = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(c8)\n",
        "    c8 = BatchNormalization()(c8)\n",
        "    x8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    x8 = concatenate([x8,c1], axis=-1)\n",
        "\n",
        "    c9 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(x8)\n",
        "    c9 = BatchNormalization()(c9)\n",
        "    c9 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')(c9)\n",
        "    c9 = BatchNormalization()(c9)\n",
        "    c10 = Conv2D(filters=2, kernel_size=(1, 1), strides=(1, 1), padding='valid', activation='softmax')(c9)\n",
        "\n",
        "    model = Model(inputs=[in_layer], outputs=[c10])\n",
        "    weights = K.sum(model.get_weights()[0])\n",
        "\n",
        "    hp_gamma = hp.Int('gamma', min_value=1, max_value=9, step=1)\n",
        "    adm = optimizers.Adam(learning_rate=1e-3)\n",
        "    model.compile(loss=focalLoss(gamma=hp_gamma, weights=weights), optimizer=adm, metrics=['accuracy',iou_metric,\n",
        "                                                                          metrics.Precision(), metrics.Recall()])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6t9BGWCw-3vJ"
      },
      "outputs": [],
      "source": [
        "tuner = keras_tuner.Hyperband(base_model,\n",
        "                     objective='val_loss',\n",
        "                     max_epochs=8,\n",
        "                     factor=3,\n",
        "                     directory='/content/Drive/MyDrive/unet_dir',\n",
        "                     project_name='focal_unet_algo1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GV8HEM7m-5YO"
      },
      "outputs": [],
      "source": [
        "tuner.search(train_images_tunner,train_masks_cat_tunner,\n",
        "             batch_size=8,\n",
        "             epochs=20,\n",
        "             validation_data=(valid_images_tunner,valid_masks_cat_tunner)\n",
        "             )\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The  Optimal Gamma\n",
        "is {best_hps.get('gamma')}.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9Rq2uDw-8g3"
      },
      "outputs": [],
      "source": [
        "best_gamma = best_hps['gamma']\n",
        "\n",
        "print (f\"best_gamma : {best_gamma}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5eHrx4OAG-U"
      },
      "source": [
        "**Paper 2 code start from Here **"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(images, masks):\n",
        "    final_images, final_masks = list(), list()\n",
        "    for e, item in enumerate(images):\n",
        "        image = cv2.imread(images[e]) # , cv2.IMREAD_GRAYSCALE#we use cv2 because PIL didnt load the image and we convert later to PIL image\n",
        "        image = image/255.\n",
        "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        final_images.append(image)\n",
        "\n",
        "        mask = cv2.imread(masks[e], cv2.IMREAD_GRAYSCALE)#we use cv2 because PIL didnt load the image and we convert later to PIL image\n",
        "        mask = mask/255.\n",
        "        # mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
        "        final_masks.append(mask)\n",
        "\n",
        "    return np.array(final_images), np.array(final_masks)"
      ],
      "metadata": {
        "id": "zR0nq-FMRXAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images, train_masks = read_data(train_img, train_masks)\n",
        "valid_images, valid_masks = read_data(valid_img, valid_masks)\n",
        "test_images, test_masks = read_data(test_img, test_masks)"
      ],
      "metadata": {
        "id": "vlbc64JbRZVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_kZpCR5RdRc",
        "outputId": "302f8c48-db66-4c75-d58b-653578128959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(490, 288, 384, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# converting all mask pixels other than 0 to 1\n",
        "for e,mask in enumerate(train_masks):\n",
        "    train_masks[e][train_masks[e]>0] = 1\n",
        "\n",
        "for e,mask in enumerate(valid_masks):\n",
        "    valid_masks[e][valid_masks[e]>0] = 1\n",
        "\n",
        "for e,mask in enumerate(test_masks):\n",
        "    test_masks[e][test_masks[e]>0] = 1"
      ],
      "metadata": {
        "id": "7wl1sQCWRgwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oZCFJsLAf3F"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input, BatchNormalization, MaxPooling2D, Conv2DTranspose, concatenate\n",
        "from keras.layers import Conv2D\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_masks_cat = to_categorical(train_masks, num_classes=n_classes)\n",
        "test_masks_cat = to_categorical(test_masks, num_classes=n_classes)\n",
        "valid_masks_cat = to_categorical(valid_masks, num_classes=n_classes)"
      ],
      "metadata": {
        "id": "ZZAz9mbRRbHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzIOjoBKA3Sr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43e8eab6-52bd-4c09-874f-6c0ce4023bc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (24.1)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (2.13.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67hpmy2YBCsD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28b386bb-25b8-4cec-e298-1fa09eba9d9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# External libraries\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Conv1D, UpSampling2D\n",
        "from tensorflow.keras.layers import concatenate, multiply, add, Activation\n",
        "from tensorflow.keras.layers import Input, concatenate\n",
        "# First, import the necessary module\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# Focus Gate\n",
        "def FocusGate(input, skip_connections, n_filters, gamma=True, stats='mean', conv_transpose=False):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    input : gating signal\n",
        "    skip_connections : incoming skip connection to be refined\n",
        "    n_filters : number of filters\n",
        "    gamma : whether to apply non-linear attention. Gamma is automatically tuned - see our latest paper\n",
        "            for more details (https://ieeexplore.ieee.org/document/9761414)\n",
        "    stats : either 'mean', 'max' or 'mean_max' to apply either global average pooling, global max pooling,\n",
        "            or both\n",
        "    conv_transpose : if false applies upsampling\n",
        "    \"\"\"\n",
        "\n",
        "    # Resize input to match number of channels for skip connections\n",
        "    resized_input = Conv2D(skip_connections.shape[-1], kernel_size=1, strides=(1, 1), padding='same', use_bias=True)(input)\n",
        "    # Resize skip connections to match image shape for input\n",
        "    resized_skip = Conv2D(skip_connections.shape[-1], kernel_size=1, strides=(2, 2), padding='same', use_bias=False)(skip_connections)\n",
        "\n",
        "    stride_x = resized_skip.shape[1] // input.shape[1]\n",
        "    stride_y = resized_skip.shape[2] // input.shape[2]\n",
        "\n",
        "    if conv_transpose:\n",
        "        resized_input = Conv2DTranspose(skip_connections.shape[-1], (stride_x, stride_y),strides=(stride_x, stride_y),padding='same')(resized_input)\n",
        "    else:\n",
        "        resized_input = UpSampling2D((stride_x,stride_y))(resized_input)\n",
        "\n",
        "    # element wise addition\n",
        "    added  = add([resized_input, resized_skip])\n",
        "\n",
        "    # perform non-linear activation\n",
        "    act = Activation('relu')(added)\n",
        "\n",
        "    # channel attention\n",
        "    channel_attention = channel_attention_block(act, stats)\n",
        "\n",
        "    # spatial attention\n",
        "    spatial_attention = spatial_attention_block(act, stats)\n",
        "\n",
        "    # combine channel and spatial weights\n",
        "    weights = multiply([channel_attention, spatial_attention])\n",
        "\n",
        "    # focal parameter for tuneable background suppression\n",
        "    if gamma:\n",
        "        weights = Gamma()(weights)\n",
        "\n",
        "    # rescale attention coefficient matrix to size of skip connection\n",
        "    stride_x_weights = skip_connections.shape[1] // weights.shape[1]\n",
        "    stride_y_weights = skip_connections.shape[2] // weights.shape[2]\n",
        "\n",
        "    # Upsample to match original skip connection resolution\n",
        "    if conv_transpose:\n",
        "        weights = Conv2DTranspose(skip_connections.shape[-1], (stride_x_weights, stride_y_weights), strides=(stride_x_weights, stride_y_weights), padding='same')(weights)\n",
        "    else:\n",
        "        weights = UpSampling2D((stride_x_weights, stride_y_weights))(weights)\n",
        "\n",
        "    # multiply skip connections by weights\n",
        "    weights = multiply([weights, skip_connections])\n",
        "\n",
        "    # perform final convolution and batch normalisation\n",
        "    output = Conv2D(skip_connections.shape[-1], kernel_size=1, strides=(1, 1), padding='same', use_bias=True)(weights)\n",
        "    output = BatchNormalization()(output)\n",
        "\n",
        "    return output\n",
        "\n",
        "# Automatically tuned gamma parameter\n",
        "class Gamma(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Gamma, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        initializer = tf.keras.initializers.Ones()\n",
        "\n",
        "        self.w = self.add_weight(\n",
        "            shape=(1,1),\n",
        "            initializer=initializer,\n",
        "            trainable=True,\n",
        "            constraint= tf.keras.constraints.NonNeg())\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = tf.clip_by_value(inputs, tf.keras.backend.epsilon(), 1.)\n",
        "        return (inputs)**self.w\n",
        "\n",
        "def generate_block(input_layer, filters, dropout_rate=0.2):\n",
        "    x = Conv2D(filters, (3, 3), padding='same', strides=1)(input_layer)\n",
        "    x = LeakyReLU(alpha=0.1)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    x = Conv2D(filters, (3, 3), padding='same', strides=1)(x)\n",
        "    x = LeakyReLU(alpha=0.1)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    return x\n",
        "\n",
        "# Gating signal\n",
        "def GatingSignal(input, skip_connections):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    input : layer prior to upsampling\n",
        "    skip_connections : incoming skip connection to be refined\n",
        "    \"\"\"\n",
        "    signal = Conv2D(skip_connections.shape[-1], (1, 1), strides=(1, 1), padding=\"same\")(input)\n",
        "    signal = LeakyReLU(alpha=0.1)(signal)\n",
        "    signal = BatchNormalization()(signal)\n",
        "    return signal\n",
        "\n",
        "def spatial_kernel_size(input, beta=1, gamma=2):\n",
        "    \"\"\"\n",
        "    The original implementation based the kernel size on the channel, but with\n",
        "    square images, it can be simplified to:\n",
        "    k = | log2(R)/gamma + b/gamma|odd\n",
        "    \"\"\"\n",
        "\n",
        "    k = int(abs(np.log2((input.shape[1])/gamma) + beta/gamma))\n",
        "    out = k if k % 2 else k + 1\n",
        "\n",
        "    return out\n",
        "\n",
        "def channel_kernel_size(input, beta=1,gamma=2):\n",
        "    \"\"\"\n",
        "    k = | log2(C)/gamma + b/gamma|odd\n",
        "    \"\"\"\n",
        "    k = int(abs(np.log2(input.shape[-1]/gamma) + beta/gamma))\n",
        "    out = k if k % 2 else k + 1\n",
        "    return out\n",
        "\n",
        "def spatial_attention_block(input, stats):\n",
        "    \"\"\" Adaptive spatial attention block\n",
        "    \"\"\"\n",
        "\n",
        "    k_size = spatial_kernel_size(input)\n",
        "\n",
        "    # generate spatial statistics\n",
        "    if (stats == 'mean') or (stats == 'mean_max'):\n",
        "        mean = tf.reduce_mean(input, axis=-1, keepdims=True)\n",
        "        t = mean\n",
        "\n",
        "    if (stats == 'max') or (stats == 'mean_max'):\n",
        "        maximum = tf.reduce_max(input, axis=-1, keepdims=True)\n",
        "        t = maximum\n",
        "\n",
        "    if (stats == 'mean_max'):\n",
        "        t = concatenate([mean,maximum], axis=-1)\n",
        "\n",
        "    t = Conv2D(1, kernel_size=k_size, padding=\"same\",activation='sigmoid', use_bias = False)(t)\n",
        "\n",
        "    return t\n",
        "\n",
        "def channel_attention_block(input, stats):\n",
        "    \"\"\" Adaptive channel attention block\n",
        "    \"\"\"\n",
        "\n",
        "    k_size = channel_kernel_size(input)\n",
        "\n",
        "    # generate channel statistics\n",
        "    if (stats == 'mean') or (stats == 'mean_max'):\n",
        "        mean = tf.reduce_mean(input, axis=[1,2], keepdims=True)\n",
        "        mean = tf.squeeze(mean,axis=(-2))\n",
        "        mean = tf.transpose(mean,perm=[0,2,1])\n",
        "        t = mean\n",
        "\n",
        "    if (stats == 'max') or (stats == 'mean_max'):\n",
        "        maximum = tf.reduce_max(input, axis=[1,2], keepdims=True)\n",
        "        maximum = tf.squeeze(maximum,axis=(-2))\n",
        "        maximum = tf.transpose(maximum,perm=[0,2,1])\n",
        "        t = maximum\n",
        "\n",
        "    if (stats == 'mean_max'):\n",
        "        t = concatenate([mean,maximum], axis=-1)\n",
        "\n",
        "    t = Conv1D(filters=1, kernel_size=k_size, padding='same', use_bias=False)(t)\n",
        "    t = tf.transpose(t,perm=[0,2,1])\n",
        "    t = tf.expand_dims(t,(-2))\n",
        "    t = tf.math.sigmoid(t)\n",
        "\n",
        "    return t\n",
        "\n",
        "# def conv_layer_2D(input, neurons):\n",
        "\n",
        "#     conv1 = Conv2D(neurons, (3, 3), padding='same', activation='relu', strides=1)(input)\n",
        "#     conv1 = BatchNormalization()(conv1)\n",
        "#     conv2 = Conv2D(neurons, (3, 3), padding='same', activation='relu', strides=1)(conv1)\n",
        "#     out = BatchNormalization()(conv2)\n",
        "\n",
        "#     return out\n",
        "\n",
        "    # Then, replace BatchNormalization with InstanceNormalization in the conv_layer_2D function\n",
        "def conv_layer_2D(input, neurons, dropout_rate=0.2):\n",
        "    conv1 = Conv2D(neurons, (3, 3), padding='same', strides=1)(input)\n",
        "    conv1 = LeakyReLU(alpha=0.1)(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Dropout(dropout_rate)(conv1)\n",
        "    conv2 = Conv2D(neurons, (3, 3), padding='same', strides=1)(conv1)\n",
        "    conv2 = LeakyReLU(alpha=0.1)(conv2)\n",
        "    out = tfa.layers.InstanceNormalization()(conv2)\n",
        "    out = Dropout(dropout_rate)(out)\n",
        "    conv3 = Conv2D(neurons, (3, 3), padding='same', strides=1)(out)\n",
        "    conv3 = LeakyReLU(alpha=0.1)(conv3)\n",
        "    out = BatchNormalization()(conv3)\n",
        "    out = Dropout(dropout_rate)(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "# U-Net model with MobileNetV2 backbone for comparison\n",
        "\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=[288,384, 3], include_top=False)\n",
        "\n",
        "# Use the activations of these layers\n",
        "layer_names = [\n",
        "    'block_1_expand_relu',   # 64x64\n",
        "    'block_3_expand_relu',   # 32x32\n",
        "    'block_6_expand_relu',   # 16x16\n",
        "    'block_13_expand_relu',  # 8x8\n",
        "    'block_16_project',      # 4x4\n",
        "]\n",
        "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "# Create the feature extraction model\n",
        "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
        "\n",
        "down_stack.trainable = False\n",
        "\n",
        "def unet_model(output_channels:int):\n",
        "    inputs = tf.keras.layers.Input(shape=[288,384, 3])\n",
        "\n",
        "    # Downsampling through the model\n",
        "    skips = down_stack(inputs)\n",
        "    x = skips[-1]\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    # Upsampling and establishing the skip connections\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "      x = up(x)\n",
        "      concat = tf.keras.layers.Concatenate()\n",
        "      x = concat([x, skip])\n",
        "      x = generate_block(x, output_channels)\n",
        "    # This is the last layer of the model\n",
        "    last = tf.keras.layers.Conv2DTranspose(\n",
        "        filters=output_channels, kernel_size=3, strides=2,\n",
        "        padding='same')  #64x64 -> 128x128\n",
        "\n",
        "    x = last(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "# Focus U-Net model with MobileNetV2 backbone\n",
        "def focus_unet_model(output_channels:int):\n",
        "    feature_map = [32, 64, 128, 256, 512]\n",
        "    inputs = tf.keras.layers.Input(shape=[288,384, 3])\n",
        "    contracting_convs = down_stack(inputs)\n",
        "    cnn_chain = contracting_convs[-1]\n",
        "    contracting_convs = reversed(contracting_convs[:-1])\n",
        "    for skip_connection, filters in zip(contracting_convs, reversed(feature_map[:-1])):\n",
        "        cnn_chain = Conv2DTranspose(filters, (2, 2), strides=(2, 2),padding='same')(cnn_chain)\n",
        "        cnn_chain = concatenate([cnn_chain, skip_connection], axis=-1)\n",
        "        cnn_chain = conv_layer_2D(cnn_chain, filters)\n",
        "        cnn_chain = Conv2D(filters, (3, 3), padding='same', strides=1)(cnn_chain)\n",
        "        cnn_chain = LeakyReLU(alpha=0.1)(cnn_chain)\n",
        "        cnn_chain = BatchNormalization()(cnn_chain)\n",
        "    last = tf.keras.layers.Conv2DTranspose(\n",
        "        filters=output_channels, kernel_size=3, strides=2,\n",
        "        padding='same')\n",
        "    x = last(cnn_chain)\n",
        "    return tf.keras.Model(inputs=inputs, outputs=x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7orkKkZJBDoy"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import metrics\n",
        "model = focus_unet_model(2)\n",
        "adm = Adam(learning_rate=1e-3)\n",
        "model.compile(loss=focalLoss(gamma=best_gamma), optimizer=adm, metrics=['accuracy',iou_metric])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGxEl0ZFBHFg"
      },
      "outputs": [],
      "source": [
        "model.fit(train_images,train_masks_cat,\n",
        "             batch_size=4,\n",
        "             epochs=12,\n",
        "             validation_data=(valid_images,valid_masks_cat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X46XjAY4BMJW"
      },
      "outputs": [],
      "source": [
        "test_predictions = model.predict(test_images)\n",
        "test_pred_classes = np.argmax(test_predictions, axis=-1)\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "\n",
        "# Flatten the arrays\n",
        "true = test_masks.flatten()\n",
        "pred = test_pred_classes.flatten()\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(true, pred)\n",
        "f1 = f1_score(true, pred)\n",
        "precision = precision_score(true, pred)\n",
        "recall = recall_score(true, pred)\n",
        "auc = roc_auc_score(true, pred)\n",
        "\n",
        "# For IoU, we can use tf.keras.metrics\n",
        "m = MeanIoU(num_classes=2)\n",
        "m.update_state(true, pred)\n",
        "iou = m.result().numpy()\n",
        "\n",
        "print('Accuracy: ', accuracy)\n",
        "print('F1 score: ', f1)\n",
        "print('Precision: ', precision)\n",
        "print('Recall: ', recall)\n",
        "print('AUC: ', auc)\n",
        "print('IoU: ', iou)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McPzRFERBQc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed9d2a7-45fe-476b-d4a5-ae5aaa7b7007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 236ms/step\n"
          ]
        }
      ],
      "source": [
        "test_predictions = model.predict(test_images)\n",
        "test_pred_classes = np.argmax(test_predictions, axis=-1)\n",
        "\n",
        "# Flatten the arrays\n",
        "t = test_masks.flatten()\n",
        "p = test_pred_classes.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zaQjy1lBSm6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
        "\n",
        "predict = model.predict(test_images)\n",
        "# Flatten the arrays\n",
        "t = test_masks.flatten()\n",
        "p = test_pred_classes.flatten()\n",
        "\n",
        "def report(y_true, y_pred):\n",
        "    print('\\n        ----------Test Data------------------')\n",
        "    print('Accuracy: {:,.2f}'.format(accuracy_score(y_true, y_pred )* 100))\n",
        "    print('Precision: {:,.2f}'.format(precision_score(y_true, y_pred, average='macro') * 100))\n",
        "    print('Recall-score: {:,.2f}'.format(recall_score(y_true, y_pred, average='macro') * 100))\n",
        "    print('F1-score: {:,.2f}'.format(f1_score(y_true, y_pred, average='macro') * 100))\n",
        "    print('AUC-score: {:,.2f}'.format(roc_auc_score(y_true, y_pred) * 100))\n",
        "    # For IoU, we can use tf.keras.metrics\n",
        "    m = MeanIoU(num_classes=2)\n",
        "    m.update_state(y_true, y_pred)\n",
        "    iou = m.result().numpy()\n",
        "    print('IoU-score: {:,.2f}'.format(iou*100))\n",
        "\n",
        "report(t,p)\n",
        "\n",
        "\n",
        "# Accuracy: 98.12\n",
        "# Precision: 94.38\n",
        "# Recall-score: 93.88\n",
        "# F1-score: 94.13\n",
        "# AUC-score: 93.88\n",
        "# IoU-score: 97.31"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}